{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4f533f-c4c8-4150-96c2-c464af12f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Get the current working directory (where you launched the notebook from)\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Assuming your notebook is in a subfolder (e.g., 'notebooks/')\n",
    "# and config.py is in the parent of that subfolder.\n",
    "parent_dir = current_dir.parent\n",
    "\n",
    "# Add the parent directory to the path\n",
    "sys.path.append(str(parent_dir))\n",
    "\n",
    "# Now your import should work\n",
    "from config import set_environment\n",
    "set_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32981c0-03c3-46a3-a86c-0dbd5b6bbbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      "--------------------------------------------------\n",
      "Status Code: 200\n",
      "Response Headers: {'Date': 'Thu, 16 Oct 2025 21:58:35 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'etag': 'W/\"387-YHZb9cp2oivhuCITEB1bLw8Bs9Y\"', 'Server': 'cloudflare', 'x-powered-by': 'Express', 'cf-cache-status': 'DYNAMIC', 'Strict-Transport-Security': 'max-age=0; preload', 'Content-Encoding': 'br', 'CF-RAY': '98fad186df20b84f-MEL'}\n",
      "\n",
      "Response Data:\n",
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"id\": \"arya\",\n",
      "      \"name\": \"Arya\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"claude-sonnet-3-5\",\n",
      "      \"name\": \"Claude 3.5 Sonnet\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"claude-sonnet-3-7\",\n",
      "      \"name\": \"Claude 3.7 Sonnet\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"claude-haiku-3-5\",\n",
      "      \"name\": \"Claude Haiku 3.5\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"claude-sonnet-4\",\n",
      "      \"name\": \"Claude Sonnet 4\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"claude-sonnet-4-5\",\n",
      "      \"name\": \"Claude Sonnet 4.5\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"deepseek-r1\",\n",
      "      \"name\": \"Deepseek R1\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"deepseek-v3\",\n",
      "      \"name\": \"Deepseek V3\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-1\",\n",
      "      \"name\": \"GPT 4.1\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o\",\n",
      "      \"name\": \"GPT 4o\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5\",\n",
      "      \"name\": \"GPT 5\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gemini-2-5-flash\",\n",
      "      \"name\": \"Gemini 2.5 Flash\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gemini-2-5-pro\",\n",
      "      \"name\": \"Gemini 2.5 Pro\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"kimi-k2\",\n",
      "      \"name\": \"Kimi K2\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sonar\",\n",
      "      \"name\": \"Perplexity Sonar\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"qwen-3-235b\",\n",
      "      \"name\": \"Qwen 3 235B\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"qwen-3-235b-thinking\",\n",
      "      \"name\": \"Qwen 3 235B Thinking\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sonar-reasoning\",\n",
      "      \"name\": \"Sonar Reasoning\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3\",\n",
      "      \"name\": \"o3\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini\",\n",
      "      \"name\": \"o3 Mini\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini\",\n",
      "      \"name\": \"o4 Mini\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get list of gab.ai models\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = os.environ[\"GAB_AI_BASE_URL\"] +  \"/models\"\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]  # Replace with your actual Gab AI API key\n",
    "\n",
    "# Set up the headers with authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Make the GET request\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    response.raise_for_status()  # This will raise an exception for 4xx/5xx errors\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Print the results in a readable format\n",
    "    print(\"API Response:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Response Headers: {dict(response.headers)}\")\n",
    "    print(\"\\nResponse Data:\")\n",
    "    print(json.dumps(data, indent=2))  # Pretty print the JSON\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        print(f\"Status Code: {e.response.status_code}\")\n",
    "        print(f\"Error Response: {e.response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5b070c-3b2b-4309-8573-7286c1f58de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm functioning well, thank you for asking. I'm always ready to assist with whatever you needâ€”always glad to engage in good conversation. How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "# test gab.ai API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url = os.environ[\"GAB_AI_BASE_URL\"],\n",
    ")\n",
    "\n",
    "# use the /v1/models GET endpoint to see available models\n",
    "# like 'arya' or 'gpt-4o'\n",
    "response = client.chat.completions.create(\n",
    "    model=\"arya\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82683ff1-b1ea-4356-97cf-fc549dbac1cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<title>Error</title>\n</head>\n<body>\n<pre>Cannot POST /v1/completions</pre>\n</body>\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m openai_llm = OpenAI(\n\u001b[32m      4\u001b[39m     api_key = os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     base_url = os.environ[\u001b[33m\"\u001b[39m\u001b[33mGAB_AI_BASE_URL\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = openai_llm.invoke(\u001b[33m\"\u001b[39m\u001b[33mTell me a joke about light bulbs!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_core/language_models/llms.py:392\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    393\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    394\u001b[39m             stop=stop,\n\u001b[32m    395\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    396\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    397\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    398\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    399\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    400\u001b[39m             **kwargs,\n\u001b[32m    401\u001b[39m         )\n\u001b[32m    402\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    403\u001b[39m         .text\n\u001b[32m    404\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_core/language_models/llms.py:791\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1002\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     run_managers = [\n\u001b[32m    989\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    990\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1000\u001b[39m         )\n\u001b[32m   1001\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_helper(\n\u001b[32m   1003\u001b[39m         prompts,\n\u001b[32m   1004\u001b[39m         stop,\n\u001b[32m   1005\u001b[39m         run_managers,\n\u001b[32m   1006\u001b[39m         new_arg_supported=\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[32m   1007\u001b[39m         **kwargs,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1010\u001b[39m     run_managers = [\n\u001b[32m   1011\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1012\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1020\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_core/language_models/llms.py:817\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    808\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    813\u001b[39m     **kwargs: Any,\n\u001b[32m    814\u001b[39m ) -> LLMResult:\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    816\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m    818\u001b[39m                 prompts,\n\u001b[32m    819\u001b[39m                 stop=stop,\n\u001b[32m    820\u001b[39m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[32m    821\u001b[39m                 run_manager=run_managers[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    822\u001b[39m                 **kwargs,\n\u001b[32m    823\u001b[39m             )\n\u001b[32m    824\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    825\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    826\u001b[39m         )\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    828\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/langchain_openai/llms/base.py:453\u001b[39m, in \u001b[36mBaseOpenAI._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m     choices.append(\n\u001b[32m    438\u001b[39m         {\n\u001b[32m    439\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: generation.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m         }\n\u001b[32m    451\u001b[39m     )\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.create(prompt=_prompts, **params)\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    455\u001b[39m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[32m    456\u001b[39m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[32m    457\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/openai/resources/completions.py:541\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    514\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    540\u001b[39m ) -> Completion | Stream[Completion]:\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    542\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    543\u001b[39m         body=maybe_transform(\n\u001b[32m    544\u001b[39m             {\n\u001b[32m    545\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    546\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m    547\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m: best_of,\n\u001b[32m    548\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mecho\u001b[39m\u001b[33m\"\u001b[39m: echo,\n\u001b[32m    549\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    550\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    551\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    552\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    553\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    554\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    555\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    556\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    557\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    558\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m    559\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msuffix\u001b[39m\u001b[33m\"\u001b[39m: suffix,\n\u001b[32m    560\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    561\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    562\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    563\u001b[39m             },\n\u001b[32m    564\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m    565\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m    566\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m    567\u001b[39m         ),\n\u001b[32m    568\u001b[39m         options=make_request_options(\n\u001b[32m    569\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    570\u001b[39m         ),\n\u001b[32m    571\u001b[39m         cast_to=Completion,\n\u001b[32m    572\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    573\u001b[39m         stream_cls=Stream[Completion],\n\u001b[32m    574\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: <!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<title>Error</title>\n</head>\n<body>\n<pre>Cannot POST /v1/completions</pre>\n</body>\n</html>"
     ]
    }
   ],
   "source": [
    "# invoke using langchain LLM method (doesn't work with GAB AI)\n",
    "from langchain_openai import OpenAI\n",
    "openai_llm = OpenAI(\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url = os.environ[\"GAB_AI_BASE_URL\"],\n",
    ")\n",
    "response = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9aca349-56a4-4934-a70b-38599a34ab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# mock LLM calls (for development)\n",
    "from langchain_community.llms import FakeListLLM\n",
    "# create fake LLM that always returns the same response\n",
    "fake_llm = FakeListLLM(responses=[\"Hello\"])\n",
    "result = fake_llm.invoke(\"Any input will return Hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68c71249-4037-4829-9eb1-b31281f38c29",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenAI' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m messages = [\n\u001b[32m     11\u001b[39m     SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre a helpful programming assistant\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWrite a Python function to calculate factorial\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m ]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# response = chat.invoke(messages)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m response = client.chat.completions.create(\n\u001b[32m     16\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33marya\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     messages=messages\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/langchain_ai/lib/python3.11/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OpenAI' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "# use langchain chat model (not working)\n",
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "client = OpenAI(\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url = os.environ[\"GAB_AI_BASE_URL\"],\n",
    "    model = \"arya\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful programming assistant\"),\n",
    "    HumanMessage(content=\"Write a Python function to calculate factorial\")\n",
    "]\n",
    "# response = chat.invoke(messages)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"arya\",\n",
    "    messages=messages\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fa20f-e8c5-4d08-9457-8591c3f3a132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
